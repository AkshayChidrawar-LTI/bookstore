{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c533e15c-5e80-422b-a7e9-bbc70ac202d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "kafka_raw = source_path+'kafka-raw/'\n",
    "#Below schemas are used to impose on source data feed. Bronze tables need not be created in advance like silver tables, because Bronze tables are not used in MERGE operation, but used to capture stream data for respective topics. This operation only requires schema definitions to be imposed on source.  \n",
    "SchemaImposedOn_record = 'key BINARY, value BINARY, topic STRING, partition LONG, offset LONG, timestamp long'\n",
    "SchemaImposedOn_books = 'book_id STRING, title STRING, author STRING, price DOUBLE, updated TIMESTAMP'\n",
    "SchemaImposedOn_customers = 'customer_id STRING, email STRING, first_name STRING, last_name STRING, gender STRING, city STRING, country_code STRING, row_status STRING, row_time timestamp'\n",
    "SchemaImposedOn_orders = 'order_id STRING, order_timestamp timestamp, customer_id STRING, quantity BIGINT, total BIGINT, books ARRAY<STRUCT<book_id STRING, quantity BIGINT, subtotal BIGINT>>'\n",
    "\n",
    "def generateReadStream():\n",
    "    readQuery = (\n",
    "        spark.readStream\n",
    "        .format('cloudFiles')\n",
    "        .option('cloudFiles.format', 'json')\n",
    "        .schema(SchemaImposedOnSource)\n",
    "        .load(kafka_raw)\n",
    "        .select(\n",
    "            F.col('topic')\n",
    "            ,F.col('key').cast('string')\n",
    "            ,(F.col('timestamp')/1000).cast('timestamp').alias('create_ts')\n",
    "            ,F.input_file_name().alias('Source_file')\n",
    "            ,F.current_timestamp().alias('insert_ts')\n",
    "            ,F.col('value').cast('string')\n",
    "            )\n",
    "    )\n",
    "    return readQuery\n",
    "\n",
    "def writeData_bronze(readQuery,topic_name,bronze_tbl_name):\n",
    "    write_bronze_Topic = (\n",
    "        readQuery\n",
    "        .filter(F.col('topic')==topic_name)\n",
    "        .withColumn('v',F.from_json(F.col('value'),getValue_ForGlobalVar('RecordSchema_',Topic)))\n",
    "        .select('key','create_ts','Source_file','insert_ts','v.*')\n",
    "        .writeStream\n",
    "        .option('checkpointLocation',getValue_ForGlobalVar('tblCheckpoint_',tblID))\n",
    "        .option('mergeSchema',True)\n",
    "        .trigger(availableNow=True)\n",
    "        .table(tbl_name)\n",
    "    )\n",
    "    write_bronze_Topic.awaitTermination()\n",
    "    \n",
    "    print(\n",
    "    'Source\\n',kafka_raw \n",
    "    ,'\\nSchema\\n',getValue_ForGlobalVar('RecordSchema_',Topic)\n",
    "    ,'\\nDest\\n',getValue_ForGlobalVar('tbl_',tblID),'\\n'\n",
    "    )\n",
    "\n",
    "def process_bronze_books():\n",
    "    readQuery = generateReadStream()\n",
    "    writeData_bronze(readQuery,'books','bronze_books')\n",
    "    \n",
    "def process_bronze_customers():\n",
    "    readQuery = generateReadStream()\n",
    "    writeData_bronze(readQuery,'customers','bronze_customers')\n",
    "\n",
    "def process_bronze_orders():\n",
    "    readQuery = generateReadStream()\n",
    "    writeData_bronze(readQuery,'orders','bronze_orders')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Processor",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
