{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c60d14d4-03d3-4d38-a8a3-c015f6fe31cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1a73d084-b400-4cd1-bce1-536ff17c7848",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_current_timestamp():\n",
    "    current_ts_sqlcol = F.from_utc_timestamp(F.current_timestamp(),'Asia/Kolkata')\n",
    "    current_ts_dtobj = datetime.now(pytz.timezone('Asia/Kolkata')) \n",
    "    current_ts_str = current_ts_dtobj.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return current_ts_sqlcol,current_ts_str\n",
    "\n",
    "  # Convert dtobject > string > sqlcol using to_timestamp \n",
    "  # (sqlcol can be used in Spark df directly, but not in spark sql; need to reconstruct again using to_timestamp during runtime)\n",
    "  # current_ts_sqlcolFromStr = F.to_timestamp(current_ts_str,'yyyy-MM-dd HH:mm:ss')\n",
    "  # print(type(current_ts_dtobj)) --> datetime\n",
    "  # print(type(current_ts_sqlcolFromStr)) --> sqlcol\n",
    "  # print(type(current_ts_sqlcol)) --> sqlcol\n",
    "  # print(type(current_ts_str)) --> string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "29e46edf-6213-41cb-b47c-a675ed2edefc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def raw_feed(spark,path_,schema_,lakeflow=1):\n",
    "    insert_ts_sqlcol,_ = get_current_timestamp()\n",
    "    if lakeflow:\n",
    "        feed = (\n",
    "            spark.readStream\n",
    "            .format('json')\n",
    "            .option('recursiveFileLookup','false')\n",
    "            .schema(schema_)\n",
    "            .load(path_)\n",
    "        )\n",
    "    else:\n",
    "        feed = (\n",
    "            spark.read\n",
    "            .format('json')\n",
    "            .option('recursiveFileLookup','false')\n",
    "            .schema(schema_)\n",
    "            .load(path_)\n",
    "        )\n",
    "    return (\n",
    "        feed\n",
    "        .select(\n",
    "            F.col('topic')\n",
    "            ,F.col('key').alias('key_encoded')\n",
    "            ,F.col('value').alias('value_encoded')\n",
    "            ,F.col('partition')\n",
    "            ,F.col('offset')\n",
    "            ,F.col('timestamp')\n",
    "            ,F.col('_metadata.file_path').alias('source_file')\n",
    "            )\n",
    "        .withColumn('key',F.col('key_encoded').cast('string'))\n",
    "        .withColumn('value',F.col('value_encoded').cast('string'))\n",
    "        .withColumn('create_ts',(F.timestamp_millis(F.col('timestamp'))))\n",
    "        .withColumn('insert_ts',insert_ts_sqlcol)\n",
    "        .select('topic','key','value','create_ts','source_file','insert_ts')\n",
    "    )\n",
    "\n",
    "def topics_feed(feed,name_,schema_):\n",
    "    insert_ts_sqlcol,_ = get_current_timestamp()\n",
    "    return (\n",
    "        feed\n",
    "        .filter(F.col('topic') == name_)\n",
    "        .select('value','source_file')\n",
    "        .withColumn('v',F.from_json(F.col('value'),schema_))\n",
    "        .withColumn('insert_ts',insert_ts_sqlcol)\n",
    "        .select('v.*','source_file','insert_ts')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09459e6c-1132-4ee6-b88a-eadefde98f91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ingest_data(source_dir,target_dir,prefix):\n",
    "    files = dbutils.fs.ls(target_dir)\n",
    "    json_files = [\n",
    "        f.name for f in files\n",
    "        if f.name.startswith(prefix) and f.name.endswith('.json')\n",
    "    ]\n",
    "    if json_files:\n",
    "        indices = [\n",
    "            int(f.rsplit('.', 1)[0].split('_')[-1])\n",
    "            for f in json_files\n",
    "        ]\n",
    "        max_index = max(indices)\n",
    "    else:\n",
    "        max_index = 0\n",
    "    next_index = str(max_index + 1).zfill(2)\n",
    "    source_file = f\"{source_dir}/{prefix}_{next_index}.json\"\n",
    "    target_file = f\"{target_dir}/{prefix}_{next_index}.json\"\n",
    "    dbutils.fs.cp(source_file,target_file)\n",
    "\n",
    "def delete_file(file_name):\n",
    "    dbutils.fs.rm(file_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
