{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26c0a743-bc4d-4a24-8040-27f0ad2032a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType,StructField,StringType,DoubleType,TimestampType, ArrayType,LongType,BinaryType\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "49929b37-8f96-4d34-b227-c1f58dc72e56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def parse_value_to_row(value,schema):\n",
    "    parsed_df = spark.createDataFrame([{\"value\":json.dumps(value)}])\\\n",
    "        .withColumn(\"parsed\",F.from_json(F.col(\"value\"),schema))\\\n",
    "        .select(\"parsed.*\")\n",
    "    return parsed_df.collect()[0].asDict()\n",
    "\n",
    "def process_feed():\n",
    "    feed = spark.sql(f\"\"\"\n",
    "                     select *\n",
    "                     from bookstore.bronze.feeds\n",
    "                     where insert_ts > (\n",
    "                         select max(insert_ts)\n",
    "                         from bookstore.silver.books\n",
    "                         )\n",
    "                         \"\"\")\n",
    "\n",
    "    insert_ts = F.from_utc_timestamp(F.current_timestamp(),'Asia/Kolkata')\n",
    "\n",
    "    topic_map = {\n",
    "        'books': ('bookstore.silver.books', schema_books),\n",
    "        'customers': ('bookstore.silver.customers', schema_customers),\n",
    "        'orders': ('bookstore.silver.orders', schema_orders)\n",
    "    }\n",
    "\n",
    "    for row in feed.collect():\n",
    "        row_dict = row.asDict()\n",
    "        topic = row_dict['topic']\n",
    "        if topic in topic_map:\n",
    "            table,schema = topic_map[topic]\n",
    "            value = row_dict['value']\n",
    "            parsed = parse_value_to_row(value,schema)\n",
    "            row_dict.pop('topic',None)\n",
    "            row_dict.pop('value',None)\n",
    "            merged_dict = {**parsed,**row_dict}\n",
    "            # spark.createDataFrame([merged_dict])\\\n",
    "            #     .write\\\n",
    "            #     .format('delta')\\\n",
    "            #     .mode('append')\\\n",
    "            #     .saveAsTable(table)\n",
    "\n",
    "        else:\n",
    "            quarantine = spark.createDataFrame([row_dict])\n",
    "            quarantine \\\n",
    "                .write \\\n",
    "                .format('delta') \\\n",
    "                .mode('append') \\\n",
    "\n",
    "                .saveAsTable('bookstore.bronze.quarantines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "372eabc6-faa8-48d0-ac24-665a157272ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Workspace/Users/akshay.chidrawar@ltimindtree.com/bookstore/transformations')\n",
    "from raw_to_bronze_input import schema_feeds,schema_books,schema_customers,schema_orders\n",
    "\n",
    "def drop_columns_from_schema(schema, columns_to_drop):\n",
    "    new_fields = [\n",
    "        field for field in schema.fields\n",
    "        if field.name not in columns_to_drop\n",
    "    ]\n",
    "    return StructType(new_fields)\n",
    "\n",
    "topic_map = {\n",
    "        'books': ('bookstore.silver.books',schema_books),\n",
    "        'customers': ('bookstore.silver.customers',schema_customers),\n",
    "        'orders': ('bookstore.silver.orders',schema_orders)\n",
    "    }\n",
    "\n",
    "if 1==1:\n",
    "    feed = spark.sql(f\"\"\"\n",
    "                    select *\n",
    "                    from bookstore.bronze.feeds\"\"\").limit(10)\n",
    "    for row in feed.collect():\n",
    "        row_dict = row.asDict()\n",
    "        topic = row_dict['topic']\n",
    "        if topic in topic_map:\n",
    "            table,schema = topic_map[topic]\n",
    "            value = row_dict['value']\n",
    "            parsed = parse_value_to_row(value,schema)\n",
    "            row_dict.pop('topic',None)\n",
    "            row_dict.pop('value',None)\n",
    "            schema = drop_columns_from_schema(schema,['create_ts','value'])\n",
    "            merged_dict = {**parsed,**row_dict}\n",
    "            df = spark.createDataFrame([merged_dict],schema)\n",
    "            display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6c86e9c7-6382-46fc-90c0-a7ca38b052e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "value={'order_id':'000000003996','order_timestamp':'2021-12-29 14:03:00','customer_id':'C00159','quantity':2,'total':73,'books':[{'book_id':'B09','quantity':1,'subtotal':24},{'book_id':'B01','quantity':1,'subtotal':49}]}\n",
    "value_str = json.dumps(value)\n",
    "display((value_str))\n",
    "\n",
    "schema= StructType([\n",
    "    StructField('order_id', StringType()),\n",
    "    StructField('order_timestamp', TimestampType()),\n",
    "    StructField('customer_id', StringType()),\n",
    "    StructField('quantity', LongType()),\n",
    "    StructField('total', LongType()),\n",
    "    StructField('books', ArrayType(StructType([\n",
    "        StructField('book_id', StringType()),\n",
    "        StructField('quantity', LongType()),\n",
    "        StructField('subtotal', LongType())\n",
    "        ])))\n",
    "])\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [{\"value\": value_str}]\n",
    ")\n",
    "\n",
    "parsed = df.select(\n",
    "    F.from_json(F.col(\"value\"),schema).alias(\"parsed\")\n",
    ").select(\"parsed.*\")\n",
    "\n",
    "display(parsed_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d8b8b4d-be6d-4d66-9643-b0b8279532c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from raw_to_bronze import *\n",
    "from raw_to_bronze_input import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b42afec5-7252-47c8-a59b-eeda77726df2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "feed = raw_feed(spark=spark,path_=path_feed,schema_=schema_feed)\n",
    "books = topics_feed(feed=feed,name_='books',schema_=schema_books)\n",
    "customers = topics_feed(feed=feed,name_='customers',schema_=schema_customers)\n",
    "orders = topics_feed(feed=feed,name_='orders',schema_=schema_orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3fe386dc-9bd6-41f8-9146-f54cbb35de72",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759668305852}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(feed)\n",
    "display(books)\n",
    "display(customers)\n",
    "display(orders)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8434604424038950,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
